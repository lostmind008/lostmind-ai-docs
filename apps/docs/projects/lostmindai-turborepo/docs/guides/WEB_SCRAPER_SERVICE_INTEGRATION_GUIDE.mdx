<Info>
This content was automatically extracted from LostMindAI-TurboRepo.
For the most up-to-date information, refer to the source project.
</Info>

# 🕷️ LostMind AI Web Scraper Service Integration Guide

> **[📚 Documentation Hub](../README.md)** | **[🏠 Main README](../../README.md)** | **[🔧 All Guides](../README.md#integration-guides)** | **[⚡ CLI Tools](CLI_DEVELOPMENT_TOOLKIT.md)**

**SOURCE PROJECT**: `/Users/sumitm1/Documents/New Ongoing Projects/New WebScraper/new ai scraper agent`

**TARGET LOCATION**: `services/crawler-engine/` within LostMind AI TurboRepo

## 🎯 Mission Overview

You are tasked with migrating the **AI-Powered Web Scraper Agent** into the TurboRepo monorepo structure as a scalable FastAPI microservice. This will serve as the core crawling engine for the chatcrawler app and provide web data extraction capabilities across the platform.

## 🏆 Integration Objectives

### Primary Goals
1. **Migrate** web scraper to `services/crawler-engine/` as FastAPI service
2. **Integrate** with shared AI clients for intelligent content extraction
3. **Implement** robust queue system for async scraping operations
4. **Connect** to shared database for storing scraped data and metadata
5. **Deploy** to Google Cloud Run for scalable, containerized operation

### Success Criteria
- ✅ FastAPI service runs successfully in TurboRepo ecosystem
- ✅ Integrates with @lostmind/ai-clients for content processing
- ✅ Uses @lostmind/db for data persistence and metadata storage
- ✅ Implements queue system for handling concurrent scraping jobs
- ✅ Respects rate limiting and website scraping policies
- ✅ Deployed and scalable on Google Cloud Run

## 🏗️ Target Architecture Integration

### Current TurboRepo Service Context
```
services/crawler-engine/      # ← YOUR DESTINATION
├── src/
│   ├── main.py              # FastAPI application entry
│   ├── api/
│   │   ├── v1/
│   │   │   ├── scraper.py   # Scraping endpoints
│   │   │   ├── jobs.py      # Job management endpoints
│   │   │   └── health.py    # Health check endpoints
│   ├── core/
│   │   ├── config.py        # Configuration management
│   │   ├── scraper.py       # Core scraping logic
│   │   ├── processor.py     # Content processing
│   │   └── queue.py         # Queue management
│   ├── models/
│   │   ├── scrape_job.py    # Scraping job models
│   │   ├── scraped_data.py  # Data models
│   │   └── responses.py     # API response models
│   └── services/
│       ├── scraper_service.py # Scraping business logic
│       ├── ai_service.py      # AI integration service
│       └── storage_service.py # Data storage service
├── tests/                   # Test suite
├── docker/
│   ├── Dockerfile          # Production container
│   └── docker-compose.yml  # Local development
├── requirements.txt        # Python dependencies
├── pyproject.toml         # Python project configuration
└── README.md
```

### Web Scraper Tech Stack
- **Framework**: FastAPI with async support
- **Scraping**: BeautifulSoup4, Scrapy, Playwright for JavaScript sites
- **AI Integration**: Google Generative AI via shared client abstraction
- **Queue System**: Redis with Celery or RQ for job processing
- **Database**: PostgreSQL via @lostmind/db for data persistence
- **Containerization**: Docker for Cloud Run deployment
- **Monitoring**: Structured logging with correlation IDs

## 🔄 Step-by-Step Migration Process

### Phase 1: Project Analysis & Architecture Planning

1. **Create Integration Branch**
   ```bash
   cd /Users/sumitm1/Documents/New\ Ongoing\ Projects/New\ WebScraper/new\ ai\ scraper\ agent
   git checkout -b integration/turbo-repo-crawler-service
   ```

2. **Analyze Current Scraper Implementation**
   - **Scraping Strategy**: Document current scraping approach (requests, selenium, etc.)
   - **Data Extraction**: Identify how content is currently parsed and processed
   - **AI Integration**: Analyze existing AI/ML usage for content processing
   - **Storage Patterns**: Document current data storage and retrieval methods
   - **Error Handling**: Review current retry logic and failure management
   - **Rate Limiting**: Assess current politeness policies and rate limiting

3. **Define Integration Requirements**
   ```python
   # Document current scraper capabilities
   class ScraperCapabilities:
       supported_sites: List[str]           # Domains that can be scraped
       javascript_rendering: bool           # Can handle JS-heavy sites
       rate_limiting: Dict[str, int]       # Rate limits per domain
       data_extraction: List[str]          # Types of data extracted
       ai_processing: bool                 # Uses AI for content processing
       async_support: bool                 # Supports concurrent operations
       authentication_handling: bool       # Can handle login-required sites
       respect_robots_txt: bool            # Follows robots.txt policies
   ```

### Phase 2: FastAPI Service Architecture

1. **Core FastAPI Setup**
   ```python
   # services/crawler-engine/src/main.py
   from fastapi import FastAPI, BackgroundTasks, Depends
   from fastapi.middleware.cors import CORSMiddleware
   from contextlib import asynccontextmanager
   import uvicorn

   from src.core.config import get_settings
   from src.api.v1 import scraper, jobs, health
   from src.core.queue import initialize_queue
   from src.services.ai_service import AIService

   @asynccontextmanager
   async def lifespan(app: FastAPI):
       # Startup
       await initialize_queue()
       yield
       # Shutdown
       pass

   def create_app() -> FastAPI:
       settings = get_settings()
       
       app = FastAPI(
           title="LostMind AI Crawler Engine",
           description="AI-Powered Web Scraping Service",
           version="1.0.0",
           lifespan=lifespan
       )

       # CORS configuration for TurboRepo apps
       app.add_middleware(
           CORSMiddleware,
           allow_origins=["http://localhost:3000", "https://*.lostmindai.com"],
           allow_credentials=True,
           allow_methods=["*"],
           allow_headers=["*"],
       )

       # Include API routers
       app.include_router(health.router, prefix="/health", tags=["health"])
       app.include_router(scraper.router, prefix="/api/v1/scraper", tags=["scraper"])
       app.include_router(jobs.router, prefix="/api/v1/jobs", tags=["jobs"])

       return app

   app = create_app()

   if __name__ == "__main__":
       uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
   ```

2. **Configuration Management**
   ```python
   # services/crawler-engine/src/core/config.py
   from pydantic import BaseSettings, Field
   from typing import Optional
   import os

   class Settings(BaseSettings):
       # Database
       database_url: str = Field(..., env="DATABASE_URL")
       
       # AI Services (using coral-muse project)
       google_api_key: str = Field(..., env="GOOGLE_GENERATIVE_AI_API_KEY")
       gcp_project_id: str = Field("coral-muse-469919-m0", env="GCP_PROJECT_ID")
       
       # Queue System
       redis_url: str = Field("redis://localhost:6379", env="REDIS_URL")
       celery_broker_url: str = Field(..., env="CELERY_BROKER_URL")
       
       # Scraping Configuration
       default_rate_limit: int = Field(1, env="DEFAULT_RATE_LIMIT")  # requests per second
       max_concurrent_jobs: int = Field(10, env="MAX_CONCURRENT_JOBS")
       request_timeout: int = Field(30, env="REQUEST_TIMEOUT")
       
       # Security
       api_key: Optional[str] = Field(None, env="CRAWLER_API_KEY")
       allowed_domains: str = Field("*", env="ALLOWED_DOMAINS")
       
       class Config:
           env_file = ".env"
           case_sensitive = False

   _settings = None

   def get_settings() -> Settings:
       global _settings
       if _settings is None:
           _settings = Settings()
       return _settings
   ```

### Phase 3: Scraping Engine Implementation

1. **Core Scraper Service**
   ```python
   # services/crawler-engine/src/core/scraper.py
   import asyncio
   import aiohttp
   from bs4 import BeautifulSoup
   from playwright.async_api import async_playwright
   from urllib.robotparser import RobotFileParser
   from typing import Dict, List, Optional, Any
   import time
   from dataclasses import dataclass
   from urllib.parse import urljoin, urlparse

   @dataclass
   class ScrapeResult:
       url: str
       title: Optional[str]
       content: str
       metadata: Dict[str, Any]
       links: List[str]
       images: List[str]
       status_code: int
       scraped_at: str
       processing_time: float

   class IntelligentScraper:
       def __init__(self, ai_service, rate_limiter):
           self.ai_service = ai_service
           self.rate_limiter = rate_limiter
           self.session = None
           self.playwright = None
           self.browser = None

       async def __aenter__(self):
           self.session = aiohttp.ClientSession(
               timeout=aiohttp.ClientTimeout(total=30)
           )
           return self

       async def __aexit__(self, exc_type, exc_val, exc_tb):
           if self.session:
               await self.session.close()
           if self.browser:
               await self.browser.close()
           if self.playwright:
               await self.playwright.stop()

       async def can_scrape(self, url: str) -> bool:
           """Check if URL can be scraped according to robots.txt"""
           try:
               parsed_url = urlparse(url)
               robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
               
               async with self.session.get(robots_url) as response:
                   if response.status == 200:
                       robots_txt = await response.text()
                       rp = RobotFileParser()
                       rp.set_url(robots_url)
                       rp.read()
                       return rp.can_fetch("*", url)
           except:
               pass
           return True

       async def scrape_static_content(self, url: str) -> ScrapeResult:
           """Scrape static content using aiohttp + BeautifulSoup"""
           start_time = time.time()
           
           await self.rate_limiter.acquire(url)
           
           headers = {
               'User-Agent': 'LostMind AI Crawler (+https://lostmindai.com/crawler)'
           }
           
           async with self.session.get(url, headers=headers) as response:
               html = await response.text()
               soup = BeautifulSoup(html, 'html.parser')
               
               # Extract content
               title = soup.find('title')
               title_text = title.get_text().strip() if title else None
               
               # Remove script and style elements
               for script in soup(["script", "style", "nav", "header", "footer"]):
                   script.decompose()
               
               content = soup.get_text()
               clean_content = '\n'.join(line.strip() for line in content.splitlines() if line.strip())
               
               # Extract links and images
               links = [urljoin(url, a.get('href')) for a in soup.find_all('a', href=True)]
               images = [urljoin(url, img.get('src')) for img in soup.find_all('img', src=True)]
               
               # AI-powered content enhancement
               enhanced_content = await self.ai_service.enhance_content(clean_content, url)
               
               return ScrapeResult(
                   url=url,
                   title=title_text,
                   content=enhanced_content,
                   metadata={'original_length': len(html), 'cleaned_length': len(clean_content)},
                   links=links,
                   images=images,
                   status_code=response.status,
                   scraped_at=time.time(),
                   processing_time=time.time() - start_time
               )

       async def scrape_dynamic_content(self, url: str) -> ScrapeResult:
           """Scrape JavaScript-heavy sites using Playwright"""
           start_time = time.time()
           
           if not self.playwright:
               self.playwright = await async_playwright().start()
               self.browser = await self.playwright.chromium.launch(headless=True)
           
           page = await self.browser.new_page()
           
           try:
               await self.rate_limiter.acquire(url)
               await page.goto(url, wait_until='networkidle')
               
               title = await page.title()
               content = await page.inner_text('body')
               
               # Extract links and images
               links = await page.evaluate('''() => {
                   return Array.from(document.querySelectorAll('a[href]'))
                       .map(a => a.href);
               }''')
               
               images = await page.evaluate('''() => {
                   return Array.from(document.querySelectorAll('img[src]'))
                       .map(img => img.src);
               }''')
               
               # AI-powered content enhancement
               enhanced_content = await self.ai_service.enhance_content(content, url)
               
               return ScrapeResult(
                   url=url,
                   title=title,
                   content=enhanced_content,
                   metadata={'dynamic_scrape': True},
                   links=links,
                   images=images,
                   status_code=200,
                   scraped_at=time.time(),
                   processing_time=time.time() - start_time
               )
               
           finally:
               await page.close()
   ```

2. **AI Integration Service**
   ```python
   # services/crawler-engine/src/services/ai_service.py
   from typing import Dict, List, Optional
   import asyncio

   # This will be replaced with actual @lostmind/ai-clients integration
   class AIService:
       def __init__(self, ai_client):
           self.ai_client = ai_client  # From @lostmind/ai-clients

       async def enhance_content(self, content: str, url: str) -> str:
           """Use AI to enhance and structure scraped content"""
           prompt = f"""
           Please analyze and enhance the following web content scraped from {url}.
           
           Task:
           1. Remove noise and irrelevant content
           2. Extract key information and main topics
           3. Structure the content logically
           4. Identify important entities and concepts
           
           Original content:
           {content[:4000]}  # Limit content length for AI processing
           
           Return enhanced, structured content:
           """
           
           try:
               enhanced = await self.ai_client.generate_text(prompt)
               return enhanced
           except Exception as e:
               # Fallback to original content if AI processing fails
               print(f"AI enhancement failed for {url}: {e}")
               return content

       async def extract_entities(self, content: str) -> List[Dict]:
           """Extract named entities and concepts from content"""
           prompt = f"""
           Extract important entities and concepts from this content.
           Return as JSON array with entity type and value.
           
           Content: {content[:2000]}
           """
           
           try:
               entities_json = await self.ai_client.generate_text(prompt)
               # Parse and return entities
               return entities_json
           except Exception:
               return []

       async def categorize_content(self, content: str) -> List[str]:
           """Categorize content into topics/themes"""
           prompt = f"""
           Categorize this content into relevant topics/themes.
           Return as comma-separated list.
           
           Content: {content[:2000]}
           """
           
           try:
               categories = await self.ai_client.generate_text(prompt)
               return [cat.strip() for cat in categories.split(',')]
           except Exception:
               return []
   ```

### Phase 4: Queue System & Job Management

1. **Celery Queue Setup**
   ```python
   # services/crawler-engine/src/core/queue.py
   from celery import Celery
   from kombu import Queue
   import asyncio
   from typing import Dict, Any

   celery_app = Celery('crawler-engine')

   celery_app.conf.update(
       broker_url='redis://localhost:6379/0',
       result_backend='redis://localhost:6379/0',
       task_serializer='json',
       accept_content=['json'],
       result_serializer='json',
       timezone='UTC',
       enable_utc=True,
       task_routes={
           'crawler.scrape_url': {'queue': 'scraping'},
           'crawler.bulk_scrape': {'queue': 'bulk_scraping'},
       },
       task_default_queue='default',
       task_queues=(
           Queue('default'),
           Queue('scraping'),
           Queue('bulk_scraping'),
       ),
   )

   @celery_app.task(bind=True, max_retries=3)
   def scrape_url_task(self, url: str, options: Dict[str, Any] = None):
       """Celery task for scraping a single URL"""
       try:
           # Import here to avoid circular imports
           from src.services.scraper_service import ScraperService
           
           service = ScraperService()
           result = asyncio.run(service.scrape_single_url(url, options or {}))
           return result
           
       except Exception as exc:
           print(f"Scraping failed for {url}: {exc}")
           raise self.retry(exc=exc, countdown=60 * (2 ** self.request.retries))

   @celery_app.task(bind=True)
   def bulk_scrape_task(self, urls: List[str], options: Dict[str, Any] = None):
       """Celery task for bulk scraping multiple URLs"""
       try:
           from src.services.scraper_service import ScraperService
           
           service = ScraperService()
           results = asyncio.run(service.scrape_multiple_urls(urls, options or {}))
           return results
           
       except Exception as exc:
           print(f"Bulk scraping failed: {exc}")
           raise exc

   async def initialize_queue():
       """Initialize queue connections"""
       # Any async queue initialization logic
       pass
   ```

2. **API Endpoints**
   ```python
   # services/crawler-engine/src/api/v1/scraper.py
   from fastapi import APIRouter, BackgroundTasks, HTTPException, Depends
   from pydantic import BaseModel, HttpUrl
   from typing import List, Optional, Dict, Any
   import uuid

   from src.core.queue import scrape_url_task, bulk_scrape_task
   from src.services.scraper_service import ScraperService
   from src.models.responses import ScrapeResponse, JobResponse

   router = APIRouter()

   class ScrapeRequest(BaseModel):
       url: HttpUrl
       options: Optional[Dict[str, Any]] = {}
       priority: Optional[int] = 5
       callback_url: Optional[HttpUrl] = None

   class BulkScrapeRequest(BaseModel):
       urls: List[HttpUrl]
       options: Optional[Dict[str, Any]] = {}
       priority: Optional[int] = 5
       callback_url: Optional[HttpUrl] = None

   @router.post("/scrape", response_model=JobResponse)
   async def scrape_url(request: ScrapeRequest):
       """Scrape a single URL asynchronously"""
       try:
           job_id = str(uuid.uuid4())
           
           # Queue the scraping task
           task = scrape_url_task.apply_async(
               args=[str(request.url), request.options],
               task_id=job_id,
               priority=request.priority
           )
           
           return JobResponse(
               job_id=job_id,
               status="queued",
               message=f"Scraping job queued for {request.url}"
           )
           
       except Exception as e:
           raise HTTPException(status_code=500, detail=str(e))

   @router.post("/scrape/bulk", response_model=JobResponse)
   async def bulk_scrape_urls(request: BulkScrapeRequest):
       """Scrape multiple URLs asynchronously"""
       try:
           job_id = str(uuid.uuid4())
           
           # Queue the bulk scraping task
           task = bulk_scrape_task.apply_async(
               args=[request.urls, request.options],
               task_id=job_id,
               priority=request.priority
           )
           
           return JobResponse(
               job_id=job_id,
               status="queued",
               message=f"Bulk scraping job queued for {len(request.urls)} URLs"
           )
           
       except Exception as e:
           raise HTTPException(status_code=500, detail=str(e))

   @router.get("/scrape/immediate")
   async def scrape_immediate(
       url: str, 
       service: ScraperService = Depends()
   ):
       """Scrape a URL immediately (for testing)"""
       try:
           result = await service.scrape_single_url(url)
           return ScrapeResponse(
               success=True,
               data=result,
               message="Scraping completed successfully"
           )
       except Exception as e:
           raise HTTPException(status_code=500, detail=str(e))
   ```

## 🛠️ Scraper-Specific CLI Tools

### **Web Scraping Development Tools**
```bash
# Playwright browser setup
npx playwright install chromium
npx playwright install-deps

# Web scraping testing tools
pip install scrapy-splash
pip install selenium-wire
pip install requests-html

# Site analysis tools
curl -I https://example.com
wget --spider --server-response https://example.com 2>&1 | grep -E "(HTTP|robots.txt)"
```

### **Queue & Job Management**
```bash
# Redis CLI for queue monitoring
redis-cli
KEYS "celery*"
LLEN "celery"

# Celery monitoring
celery -A src.core.queue.celery_app worker --loglevel=info
celery -A src.core.queue.celery_app beat --loglevel=info
celery -A src.core.queue.celery_app flower  # Web-based monitoring
```

### **Testing & Debugging**
```bash
# Test scraping capabilities
curl -X POST "http://localhost:8000/api/v1/scraper/scrape/immediate" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'

# Monitor scraping performance
docker stats crawler-engine
docker logs -f crawler-engine
```

## 🐳 Docker & Deployment Configuration

### **Dockerfile**
```dockerfile
# services/crawler-engine/docker/Dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    && rm -rf /var/lib/apt/lists/*

# Install Playwright dependencies
RUN pip install playwright
RUN playwright install chromium
RUN playwright install-deps

WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY src/ ./src/
COPY main.py .

# Create non-root user
RUN useradd --create-home --shell /bin/bash app \
    && chown -R app:app /app
USER app

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### **Cloud Run Deployment**
```bash
# Build and deploy to Google Cloud Run
gcloud builds submit --tag gcr.io/coral-muse-469919-m0/crawler-engine

gcloud run deploy crawler-engine \
  --image gcr.io/coral-muse-469919-m0/crawler-engine \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --memory 2Gi \
  --cpu 2 \
  --max-instances 10 \
  --set-env-vars DATABASE_URL=$DATABASE_URL \
  --set-env-vars GOOGLE_GENERATIVE_AI_API_KEY=$GOOGLE_GENERATIVE_AI_API_KEY
```

## 📊 Integration with TurboRepo Packages

### Database Integration (@lostmind/db)
```python
# services/crawler-engine/src/services/storage_service.py
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Dict, List
import json

class StorageService:
    def __init__(self, db_session: AsyncSession):
        self.db = db_session

    async def save_scrape_result(self, result: ScrapeResult, user_id: str = None):
        """Save scraping result to shared database"""
        scrape_record = {
            'url': result.url,
            'title': result.title,
            'content': result.content,
            'metadata': json.dumps(result.metadata),
            'scraped_at': result.scraped_at,
            'user_id': user_id,  # For multi-tenancy
            'processing_time': result.processing_time,
            'status_code': result.status_code
        }
        
        # Use shared database models from @lostmind/db
        # Implementation will depend on actual database schema
        
    async def get_scrape_history(self, user_id: str, limit: int = 100):
        """Retrieve user's scraping history"""
        # Query shared database for user's scrape history
        pass
        
    async def search_scraped_content(self, query: str, user_id: str):
        """Search through scraped content using full-text search"""
        # Implement search across scraped content
        pass
```

## ✅ Web Scraper Migration Checklist

### Pre-Migration Analysis
- [ ] Document all current scraping capabilities and supported sites
- [ ] Identify AI/ML processing currently implemented
- [ ] Analyze rate limiting and politeness policies
- [ ] Review error handling and retry mechanisms
- [ ] Document authentication and session handling

### Migration Execution
- [ ] Set up FastAPI service structure in TurboRepo
- [ ] Implement core scraping engine with BeautifulSoup and Playwright
- [ ] Integrate with @lostmind/ai-clients for content enhancement
- [ ] Set up Celery queue system with Redis
- [ ] Configure database integration with @lostmind/db
- [ ] Implement API endpoints for scraping operations
- [ ] Set up Docker containerization

### Testing & Validation
- [ ] Test scraping against various website types
- [ ] Verify AI content enhancement is working
- [ ] Test queue system and job management
- [ ] Validate rate limiting and politeness policies
- [ ] Test error handling and retry mechanisms
- [ ] Verify database integration and data persistence

### Production Deployment
- [ ] Deploy to Google Cloud Run
- [ ] Configure environment variables and secrets
- [ ] Set up monitoring and logging
- [ ] Test integration with chatcrawler app
- [ ] Configure scaling and resource limits

## 🎯 Success Metrics

### Performance Targets
- **Response Time**: <5s for most websites
- **Throughput**: 100+ URLs per minute
- **Success Rate**: >95% for standard websites
- **Queue Processing**: <10s average job wait time

### Quality Targets
- **Content Accuracy**: >90% relevant content extracted
- **AI Enhancement**: Measurable improvement in content quality
- **Error Rate**: <5% failed scraping attempts
- **Compliance**: 100% robots.txt adherence

## 🤝 Integration Communication

### When Ready for Integration
1. **Service Testing**: All endpoints working and tested
2. **Queue System**: Job processing functioning correctly
3. **AI Integration**: Content enhancement operational
4. **Database Integration**: Data persistence working
5. **Docker Deployment**: Container builds and runs successfully

### Handoff Documentation Required
- **API Documentation**: All endpoints and response formats
- **Scraping Capabilities**: Supported sites and limitations
- **Performance Metrics**: Benchmarking results
- **Deployment Guide**: Cloud Run configuration and scaling
- **Integration Status**: Database and AI client connections

---

**🎯 Your Mission**: Transform the standalone web scraper into a robust, scalable FastAPI microservice that integrates seamlessly with the TurboRepo ecosystem while maintaining high performance and respectful scraping practices.

**📞 Ready Signal**: Create a comprehensive pull request with API documentation, performance benchmarks, and full deployment configuration when the scraper service integration is complete and tested.