---
title: "README"
description: "readme for LostMindAI-TurboRepo"
category: "readme"
project: "LostMindAI-TurboRepo"
lastUpdated: "2025-09-21"
tags: "readme,main-platform"
---

# Document Processor Service

FastAPI microservice for document analysis in the LostMind AI Intelligence Hub (JustDropIt MVP).

## Overview

This service handles document upload, analysis, and result retrieval for the Intelligence Hub. It provides secure, multi-tenant document processing with support for various analysis types.

## Features

- **Secure Authentication**: JWT token validation with tenant isolation
- **File Processing**: Support for Excel (.xlsx, .xls), CSV, and PDF files
- **Analysis Types**: Accrual analysis with planned expansion to other financial analyses
- **Background Processing**: Asynchronous job processing with status tracking
- **Multi-tenant**: Complete tenant isolation and access control
- **Monitoring**: Health checks and comprehensive logging

## API Endpoints

### Health Check
```
GET /health
```

### Analysis Operations
```
POST /api/v1/analyze         # Start analysis job
GET /api/v1/analysis/{id}    # Get job status and results
GET /api/v1/jobs             # List user's jobs
```

## Installation

### Using Docker (Recommended)

```bash
# Build the container
docker build -t document-processor .

# Run the service
docker run -p 8001:8001 \
  -e DATABASE_URL="postgresql://..." \
  -e NEXTAUTH_SECRET="your-secret" \
  document-processor
```

### Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables
cp .env.example .env
# Edit .env with your configuration

# Run the service
uvicorn main:app --reload --port 8001
```

## Environment Variables

Required:
- `DATABASE_URL`: PostgreSQL connection string
- `NEXTAUTH_SECRET`: JWT signing secret

Optional:
- `ENVIRONMENT`: development/staging/production (default: development)
- `PORT`: Service port (default: 8001)
- `MAX_FILE_SIZE`: Maximum upload size in bytes (default: 100MB)
- `UPLOAD_DIRECTORY`: Temporary file storage path (default: /tmp/uploads)

## Usage

### Starting an Analysis

```bash
curl -X POST "http://localhost:8001/api/v1/analyze" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "file_url": "/path/to/document.xlsx",
    "analysis_type": "ACCRUAL_ANALYSIS",
    "parameters": {
      "sheet_name": "Financial Data"
    }
  }'
```

### Checking Job Status

```bash
curl -X GET "http://localhost:8001/api/v1/analysis/JOB_ID" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN"
```

## Analysis Types

### ACCRUAL_ANALYSIS
Analyzes financial documents for accrual-based accounting insights.

**Supported Files**: Excel (.xlsx, .xls), CSV
**Parameters**:
- `sheet_name`: Excel sheet to analyze (default: first sheet)
- `delimiter`: CSV delimiter (default: comma)
- `encoding`: File encoding (default: utf-8)

**Results**:
- Total revenue calculations
- Accrued expenses analysis
- Net accrual summaries
- Monthly breakdown reports
- Recommendations for process improvements

## Security

- **JWT Authentication**: All endpoints require valid JWT tokens
- **Tenant Isolation**: Multi-tenant data separation
- **File Validation**: MIME type and content validation
- **Rate Limiting**: Subscription-based usage limits
- **Audit Logging**: Comprehensive request/response logging

## Architecture

```
services/document-processor/
├── main.py                 # FastAPI application
├── config.py              # Configuration management
├── requirements.txt       # Python dependencies
├── Dockerfile             # Container configuration
├── middleware/            # Authentication and security
│   ├── auth.py           # JWT token verification
│   └── tenant_isolation.py # Multi-tenant access control
└── core/                 # Business logic
    ├── analyzers/        # Analysis implementations
    │   └── accrual_analyzer.py
    └── utils/           # File handling utilities
        └── file_handler.py
```

## Development

### Adding New Analysis Types

1. Create analyzer in `core/analyzers/`
2. Implement the `analyze()` method
3. Add to the analyzer mapping in `main.py`
4. Update permission checks in `middleware/auth.py`

### Testing

```bash
# Install test dependencies
pip install pytest pytest-asyncio httpx

# Run tests
pytest tests/
```

## Deployment

The service is designed for deployment on Google Cloud Run with the existing infrastructure.

### Docker Compose (Development)

```yaml
version: '3.8'
services:
  document-processor:
    build: .
    ports:
      - "8001:8001"
    environment:
      - DATABASE_URL=postgresql://...
      - NEXTAUTH_SECRET=your-secret
    volumes:
      - ./uploads:/tmp/uploads
```

## Monitoring

- Health endpoint: `/health`
- Logs: Structured JSON logging
- Metrics: Processing time, success/failure rates
- Alerts: Failed jobs, service availability

## Future Enhancements

- [ ] Integration with PropTech-Backend analysis algorithms
- [ ] Support for additional file formats (Word, PowerPoint)
- [ ] Real-time progress updates via WebSockets
- [ ] Result caching and optimization
- [ ] Advanced analytics and reporting
- [ ] Mach

---
*This content was automatically extracted from LostMindAI-TurboRepo. For the most up-to-date information, refer to the source project.*
