---
title: "LostMindAI-TurboRepo"
description: "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository."
category: "guide"
project: "LostMind AI - Project Documentation"
lastUpdated: "2025-09-21"
tags: "guide,ai-services"
---

# ğŸ•·ï¸ LostMind AI Web Scraper Service Integration Guide

> **[ğŸ“š Documentation Hub](../README.md)** | **[ğŸ  Main README](../../README.md)** | **[ğŸ”§ All Guides](../README.md#integration-guides)** | **[âš¡ CLI Tools](CLI_DEVELOPMENT_TOOLKIT.md)**

**SOURCE PROJECT**: `/Users/sumitm1/Documents/New Ongoing Projects/New WebScraper/new ai scraper agent`

**TARGET LOCATION**: `services/crawler-engine/` within LostMind AI TurboRepo

## ğŸ¯ Mission Overview

You are tasked with migrating the **AI-Powered Web Scraper Agent** into the TurboRepo monorepo structure as a scalable FastAPI microservice. This will serve as the core crawling engine for the chatcrawler app and provide web data extraction capabilities across the platform.

## ğŸ† Integration Objectives

### Primary Goals
1. **Migrate** web scraper to `services/crawler-engine/` as FastAPI service
2. **Integrate** with shared AI clients for intelligent content extraction
3. **Implement** robust queue system for async scraping operations
4. **Connect** to shared database for storing scraped data and metadata
5. **Deploy** to Google Cloud Run for scalable, containerized operation

### Success Criteria
- âœ… FastAPI service runs successfully in TurboRepo ecosystem
- âœ… Integrates with @lostmind/ai-clients for content processing
- âœ… Uses @lostmind/db for data persistence and metadata storage
- âœ… Implements queue system for handling concurrent scraping jobs
- âœ… Respects rate limiting and website scraping policies
- âœ… Deployed and scalable on Google Cloud Run

## ğŸ—ï¸ Target Architecture Integration

### Current TurboRepo Service Context
```
services/crawler-engine/      # â† YOUR DESTINATION
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # FastAPI application entry
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ scraper.py   # Scraping endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ jobs.py      # Job management endpoints
â”‚   â”‚   â”‚   â””â”€â”€ health.py    # Health check endpoints
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration management
â”‚   â”‚   â”œâ”€â”€ scraper.py       # Core scraping logic
â”‚   â”‚   â”œâ”€â”€ processor.py     # Content processing
â”‚   â”‚   â””â”€â”€ queue.py         # Queue management
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ scrape_job.py    # Scraping job models
â”‚   â”‚   â”œâ”€â”€ scraped_data.py  # Data models
â”‚   â”‚   â””â”€â”€ responses.py     # API response models
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ scraper_service.py # Scraping business logic
â”‚       â”œâ”€â”€ ai_service.py      # AI integration service
â”‚       â””â”€â”€ storage_service.py # Data storage service
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile          # Production container
â”‚   â””â”€â”€ docker-compose.yml  # Local development
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ pyproject.toml         # Python project configuration
â””â”€â”€ README.md
```

### Web Scraper Tech Stack
- **Framework**: FastAPI with async support
- **Scraping**: BeautifulSoup4, Scrapy, Playwright for JavaScript sites
- **AI Integration**: Google Generative AI via shared client abstraction
- **Queue System**: Redis with Celery or RQ for job processing
- **Database**: PostgreSQL via @lostmind/db for data persistence
- **Containerization**: Docker for Cloud Run deployment
- **Monitoring**: Structured logging with correlation IDs

## ğŸ”„ Step-by-Step Migration Process

### Phase 1: Project Analysis & Architecture Planning

1. **Create Integration Branch**
   ```bash
   cd /Users/sumitm1/Documents/New\ Ongoing\ Projects/New\ WebScraper/new\ ai\ scraper\ agent
   git checkout -b integration/turbo-repo-crawler-service
   ```

2. **Analyze Current Scraper Implementation**
   - **Scraping Strategy**: Document current scraping approach (requests, selenium, etc.)
   - **Data Extraction**: Identify how content is currently parsed and processed
   - **AI Integration**: Analyze existing AI/ML usage for content processing
   - **Storage Patterns**: Document current data storage and retrieval methods
   - **Error Handling**: Review current retry logic and failure management
   - **Rate Limiting**: Assess current politeness policies and rate limiting

3. **Define Integration Requirements**
   ```python
   # Document current scraper capabilities
   class ScraperCapabilities:
       supported_sites: List[str]           # Domains that can be scraped
       javascript_rendering: bool           # Can handle JS-heavy sites
       rate_limiting: Dict[str, int]       # Rate limits per domain
       data_extraction: List[str]          # Types of data extracted
       ai_processing: bool                 # Uses AI for content processing
       async_support: bool                 # Supports concurrent operations
       authentication_handling: bool       # Can handle login-required sites
       respect_robots_txt: bool            # Follows robots.txt policies
   ```

### Phase 2: FastAPI Service Architecture

1. **Core FastAPI Setup**
   ```python
   # services/crawler-engine/src/main.py
   from fastapi import FastAPI, BackgroundTasks, Depends
   from fastapi.middleware.cors import CORSMiddleware
   from context

---
*This content was automatically extracted from LostMind AI - Project Documentation. For the most up-to-date information, refer to the source project.*
